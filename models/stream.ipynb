{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load models\n",
    "harassment_model_path = \"harass.h5\"\n",
    "harassment_model = tf.keras.models.load_model(harassment_model_path)\n",
    "\n",
    "gender_model_path = \"gender_classification_model.h5\"\n",
    "gender_model = tf.keras.models.load_model(gender_model_path)\n",
    "\n",
    "expression_model_path = \"emotion_model.h5\"\n",
    "expression_model = tf.keras.models.load_model(expression_model_path)\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Function to process frames for harassment detection\n",
    "def process_frame_for_harassment(frame):\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    features = base_model.predict(img_array)\n",
    "    features = features.reshape(1, 7 * 7 * 512)\n",
    "    predictions = harassment_model.predict(features)\n",
    "    return np.argmax(predictions[0])\n",
    "\n",
    "# Function to process frames for gender detection\n",
    "def process_frame_for_gender(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    male_count = 0\n",
    "    female_count = 0\n",
    "    lone_woman_detected = False\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        roi_resized = cv2.resize(roi, (64, 64))\n",
    "        roi_array = img_to_array(roi_resized) / 255.0\n",
    "        roi_array = np.expand_dims(roi_array, axis=0)\n",
    "        \n",
    "        prediction = gender_model.predict(roi_array)\n",
    "        result = \"Male\" if prediction[0][0] >= 0.5 else \"Female\"\n",
    "        \n",
    "        if result == \"Male\":\n",
    "            male_count += 1\n",
    "        else:\n",
    "            female_count += 1\n",
    "            if male_count == 0:\n",
    "                lone_woman_detected = True\n",
    "        \n",
    "        # Draw rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, result, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, male_count, female_count, lone_woman_detected\n",
    "\n",
    "# Function to process frames for emotion detection\n",
    "def process_frame_for_emotion(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    detected_emotions = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        face_roi_resized = cv2.resize(face_roi, (48, 48))\n",
    "        face_roi_normalized = face_roi_resized / 255.0\n",
    "        face_roi_normalized = face_roi_normalized.astype(np.float32)\n",
    "        face_roi_rgb = cv2.cvtColor(face_roi_normalized, cv2.COLOR_GRAY2RGB)\n",
    "        face_roi_expanded = np.expand_dims(face_roi_rgb, axis=0)\n",
    "        \n",
    "        emotion_prediction = expression_model.predict(face_roi_expanded)\n",
    "        emotion_label = emotion_labels[np.argmax(emotion_prediction)]\n",
    "        detected_emotions.append(emotion_label)\n",
    "        \n",
    "        # Emotion label below the bounding box\n",
    "        cv2.putText(frame, emotion_label, (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    return frame, detected_emotions\n",
    "\n",
    "# Streamlit app\n",
    "def main():\n",
    "    st.title(\"Real-Time Detection: Harassment, Gender, and Emotion\")\n",
    "    \n",
    "    # Video input using Streamlit camera\n",
    "    use_camera = st.checkbox(\"Use Camera for Video Feed\", value=True)\n",
    "    \n",
    "    if use_camera:\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        st.sidebar.text(\"Press 'Stop' to end the video.\")\n",
    "        frame_window = st.image([])\n",
    "\n",
    "        while video_capture.isOpened():\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                st.error(\"Unable to access camera feed.\")\n",
    "                break\n",
    "\n",
    "            harassment_prediction = process_frame_for_harassment(frame)\n",
    "            frame, male_count, female_count, lone_woman_detected = process_frame_for_gender(frame)\n",
    "            frame, detected_emotions = process_frame_for_emotion(frame)\n",
    "\n",
    "            # Overlay detection results on the frame\n",
    "            if harassment_prediction == 1:\n",
    "                cv2.putText(frame, \"Harassment Detected!\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            if lone_woman_detected:\n",
    "                cv2.putText(frame, \"Lone Woman Detected\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "            # Display the video frame\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert color for Streamlit\n",
    "            frame_window.image(frame)\n",
    "\n",
    "        video_capture.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
